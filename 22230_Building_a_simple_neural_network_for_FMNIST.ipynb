{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "DiFjmrebfP_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpPYTNrMdvc3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Datasets"
      ],
      "metadata": {
        "id": "O3r01Al8fTk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "\n",
        ")\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform= Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0,torch.tensor(y), value=1))\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_y5nDOSeQ2M",
        "outputId": "999e95dc-f259-4f9d-cbc2-90d18277eaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 10037509.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 268133.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4959706.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 22586063.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Dataset for Training, Testing and Validating"
      ],
      "metadata": {
        "id": "L5cBpgFQfXoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(train_dataset)\n",
        "val_size = int(0.1 * train_size)\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size - val_size, val_size])"
      ],
      "metadata": {
        "id": "dvbRxOF9epJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYXSbmsmewRD",
        "outputId": "e3beecea-71ea-4bd9-8f65-99961fbe39cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set size: 10000\n",
            "Training set size: 54000\n",
            "Validation set size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "iPUE0sl6f1Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)             # batch size is 64\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "KFnCsKE4fv_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Neural Network"
      ],
      "metadata": {
        "id": "4FZtAMXje11m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(in_features=28*28, out_features=256)\n",
        "        self.linear2 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.linear3 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.linear4 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x): # forward prop function\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = torch.relu(self.linear3(x))\n",
        "        x = torch.softmax(self.linear4(x),dim = 1)\n",
        "        return x\n",
        "\n",
        "simple_net = SimpleNet()"
      ],
      "metadata": {
        "id": "D2wjKcn0ey9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variable initialisation for Training"
      ],
      "metadata": {
        "id": "noAfcsk7flsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(simple_net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "ser16OhZflIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "MEe-2Jj2f5Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainval_loop(train_loader, val_loader, simple_net, criterion, optimizer):\n",
        "\n",
        "    simple_net.train()\n",
        "\n",
        "\n",
        "    size = len(train_loader.dataset)\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "\n",
        "        pred = simple_net(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "                                                 # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % 100 == 0:\n",
        "            current = batch * len(X)\n",
        "            print(f\"Training loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\") #printing training loss\n",
        "\n",
        "\n",
        "    simple_net.eval()\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = 0\n",
        "    val_size = len(val_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(val_loader):\n",
        "            # Compute prediction and loss\n",
        "            pred = simple_net(X)\n",
        "            loss = criterion(pred, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                current = batch * len(X)\n",
        "                print(f\"Validation loss: {loss.item():>7f}  [{current:>5d}/{val_size:>5d}]\") #printing validation loss\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Avg. Validation loss: {avg_val_loss:>7f}\")"
      ],
      "metadata": {
        "id": "-rYlwJR0f6Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f'===EPOCH===  {t}')\n",
        "    trainval_loop(train_loader,val_loader, simple_net, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdFHzNifgu27",
        "outputId": "c1ddf122-64f3-4cf8-f4d3-b526e0d1eaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===EPOCH===  0\n",
            "Training loss: 2.300442  [    0/54000]\n",
            "Training loss: 2.179726  [ 6400/54000]\n",
            "Training loss: 2.398649  [12800/54000]\n",
            "Training loss: 2.336150  [19200/54000]\n",
            "Training loss: 2.304900  [25600/54000]\n",
            "Training loss: 2.367400  [32000/54000]\n",
            "Training loss: 2.383025  [38400/54000]\n",
            "Training loss: 2.383025  [44800/54000]\n",
            "Training loss: 2.398650  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  1\n",
            "Training loss: 2.320525  [    0/54000]\n",
            "Training loss: 2.398650  [ 6400/54000]\n",
            "Training loss: 2.336150  [12800/54000]\n",
            "Training loss: 2.336150  [19200/54000]\n",
            "Training loss: 2.351775  [25600/54000]\n",
            "Training loss: 2.367400  [32000/54000]\n",
            "Training loss: 2.336150  [38400/54000]\n",
            "Training loss: 2.414275  [44800/54000]\n",
            "Training loss: 2.398650  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  2\n",
            "Training loss: 2.304900  [    0/54000]\n",
            "Training loss: 2.351775  [ 6400/54000]\n",
            "Training loss: 2.383025  [12800/54000]\n",
            "Training loss: 2.367400  [19200/54000]\n",
            "Training loss: 2.336150  [25600/54000]\n",
            "Training loss: 2.351775  [32000/54000]\n",
            "Training loss: 2.351775  [38400/54000]\n",
            "Training loss: 2.398650  [44800/54000]\n",
            "Training loss: 2.336150  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  3\n",
            "Training loss: 2.367400  [    0/54000]\n",
            "Training loss: 2.367400  [ 6400/54000]\n",
            "Training loss: 2.336150  [12800/54000]\n",
            "Training loss: 2.383025  [19200/54000]\n",
            "Training loss: 2.336150  [25600/54000]\n",
            "Training loss: 2.383025  [32000/54000]\n",
            "Training loss: 2.398650  [38400/54000]\n",
            "Training loss: 2.398650  [44800/54000]\n",
            "Training loss: 2.445525  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  4\n",
            "Training loss: 2.304900  [    0/54000]\n",
            "Training loss: 2.351775  [ 6400/54000]\n",
            "Training loss: 2.320525  [12800/54000]\n",
            "Training loss: 2.398650  [19200/54000]\n",
            "Training loss: 2.414275  [25600/54000]\n",
            "Training loss: 2.367400  [32000/54000]\n",
            "Training loss: 2.398650  [38400/54000]\n",
            "Training loss: 2.320525  [44800/54000]\n",
            "Training loss: 2.336150  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  5\n",
            "Training loss: 2.398650  [    0/54000]\n",
            "Training loss: 2.351775  [ 6400/54000]\n",
            "Training loss: 2.336150  [12800/54000]\n",
            "Training loss: 2.351775  [19200/54000]\n",
            "Training loss: 2.414275  [25600/54000]\n",
            "Training loss: 2.336150  [32000/54000]\n",
            "Training loss: 2.383025  [38400/54000]\n",
            "Training loss: 2.367400  [44800/54000]\n",
            "Training loss: 2.367400  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  6\n",
            "Training loss: 2.336150  [    0/54000]\n",
            "Training loss: 2.320525  [ 6400/54000]\n",
            "Training loss: 2.367400  [12800/54000]\n",
            "Training loss: 2.429900  [19200/54000]\n",
            "Training loss: 2.383025  [25600/54000]\n",
            "Training loss: 2.351775  [32000/54000]\n",
            "Training loss: 2.336150  [38400/54000]\n",
            "Training loss: 2.320525  [44800/54000]\n",
            "Training loss: 2.351775  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  7\n",
            "Training loss: 2.414275  [    0/54000]\n",
            "Training loss: 2.383025  [ 6400/54000]\n",
            "Training loss: 2.429900  [12800/54000]\n",
            "Training loss: 2.383025  [19200/54000]\n",
            "Training loss: 2.336150  [25600/54000]\n",
            "Training loss: 2.367400  [32000/54000]\n",
            "Training loss: 2.398650  [38400/54000]\n",
            "Training loss: 2.351775  [44800/54000]\n",
            "Training loss: 2.383025  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  8\n",
            "Training loss: 2.398650  [    0/54000]\n",
            "Training loss: 2.414275  [ 6400/54000]\n",
            "Training loss: 2.351775  [12800/54000]\n",
            "Training loss: 2.351775  [19200/54000]\n",
            "Training loss: 2.336150  [25600/54000]\n",
            "Training loss: 2.304900  [32000/54000]\n",
            "Training loss: 2.367400  [38400/54000]\n",
            "Training loss: 2.398650  [44800/54000]\n",
            "Training loss: 2.367400  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n",
            "===EPOCH===  9\n",
            "Training loss: 2.414275  [    0/54000]\n",
            "Training loss: 2.367400  [ 6400/54000]\n",
            "Training loss: 2.398650  [12800/54000]\n",
            "Training loss: 2.336150  [19200/54000]\n",
            "Training loss: 2.383025  [25600/54000]\n",
            "Training loss: 2.336150  [32000/54000]\n",
            "Training loss: 2.304900  [38400/54000]\n",
            "Training loss: 2.367400  [44800/54000]\n",
            "Training loss: 2.367400  [51200/54000]\n",
            "Validation loss: 2.320525  [    0/ 6000]\n",
            "Avg. Validation loss: 2.360363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "bdXW1hddgsNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(test_loader, simple_net, criterion):\n",
        "    simple_net.eval()\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            pred = simple_net(X)\n",
        "            test_loss += criterion(pred, y).item()                     # testing loop for accuracy check , basically tessting the model\n",
        "\n",
        "\n",
        "            pred_labels = pred.argmax(dim=1)\n",
        "            y= y.argmax(dim=1)\n",
        "\n",
        "            correct += (pred_labels == y).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    accuracy = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):.2f}%, Avg loss: {test_loss:.8f} \\n\")"
      ],
      "metadata": {
        "id": "eKS20Ugsf-sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_loader, simple_net, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNa9Ul4Gg6Ue",
        "outputId": "385fed26-fc5d-4f17-cc21-170a3c01efbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 10.00%, Avg loss: 2.36103073 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}